{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 05: Numerical differentiation and Taylor Series\n",
    "\n",
    "**Exercise:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/ce311k/blob/main/notebooks/lectures/05_numerical_diff_taylor_series_newton_raphson/05_finite_difference_taylor_series.ipynb)\n",
    "**Solution:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/ce311k/blob/main/notebooks/lectures/05_numerical_diff_taylor_series_newton_raphson/05_finite_difference_taylor_series_solutions.ipynb)\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2G86RGPE1D0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/L4ThylfnZhg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "   * explain the definitions of forward, backward, and center divided methods for numerical differentiation\n",
    "   * find approximate values of the first derivative of continuous functions\n",
    "   * reason about the accuracy of the numbers\n",
    "   * find approximate values of the first derivative of discrete functions (given at discrete data points)\n",
    "   \n",
    "   \n",
    "Numerical differentiation is the process of finding the numerical value of a derivative of a given function at a given point.\n",
    "\n",
    "A simple two-point estimation is to compute the slope of a nearby secant line through the points $(x, f(x))$ and $(x + h, f(x + h))$. Choosing a small number $h$, $h$ represents a small change in $x$ ($h <<1$ and is positive). The slope of this line is \n",
    "\n",
    "![secant slope](https://raw.githubusercontent.com/kks32-courses/ce311k/master/notebooks/lectures/05_numerical_diff_taylor_series_newton_raphson/derivative.png)\n",
    "\n",
    "$$f^\\prime(x) \\approxeq \\lim_{h\\rightarrow 0}\\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "Three basic types are commonly considered: forward, backward, and central differences.\n",
    "\n",
    "\n",
    "![differencing schemes](https://raw.githubusercontent.com/kks32-courses/ce311k/master/notebooks/lectures/05_numerical_diff_taylor_series_newton_raphson/finite-difference-methods.png)\n",
    "\n",
    "## Forward difference\n",
    "\n",
    "\n",
    "$$f^\\prime(x) = \\frac{f(x+h) - f(x)}{h} + O(h)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward difference\n",
    "\n",
    "\n",
    "$$f^\\prime(x) = \\frac{f(x) - f(x-h)}{h} + O(h)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central difference method\n",
    "\n",
    "\n",
    "$$f^\\prime(x) = \\frac{f(x+h) - f(x-h)}{2h} + O(h^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second derivative\n",
    "$$f^{\\prime\\prime}(x) = \\frac{f(x+h) - 2 f(x) + f(x-h)}{h^2} + O(h^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy differentiate\n",
    "\n",
    "Another library, which largely builds on NumPy and provides additional functionality, is SciPy (https://www.scipy.org/). SciPy provides some  more specialised data structures and functions over NumPy. \n",
    "If you are familiar with MATLAB, NumPy and SciPy provide much of what is available in MATLAB.\n",
    "\n",
    "[scipy derivative](https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.derivative.html)\n",
    "\n",
    "```\n",
    "from scipy.misc import derivative\n",
    "\n",
    "scipy.misc.derivative(func, x0, dx=1.0, n=1, args=(), order=3)[source]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taylor series and approximations\n",
    "\n",
    "Taylor series is one the best tools maths has to offer for approximating functions. Taylor series is about taking non-polynomial functions and finding polyomials that approximate at some input. The motive here is the polynomials tend to be much easier to deal with than other functions, they are easier to compute, take derivatives, integrate, just easier overall. \n",
    "\n",
    "Taylor series of a function is an infinite sum of terms that are expressed in terms of the function's derivatives at a single point. The Taylor series of a function $f(x)$ that is infinitely differentiable at a real or complex number $a$ is the power series\n",
    "\n",
    "$$f(a)+\\frac {f'(a)}{1!} (x-a)+ \\frac{f''(a)}{2!} (x-a)^2+\\frac{f'''(a)}{3!}(x-a)^3+ \\cdots$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential energy of a simple pendulum\n",
    "\n",
    "\n",
    "To determine the potential energy of a pendulum, for that we need an expression for how high the weight of the pendulum is above its lowest point. This works out to be  $h = R(1 - \\cos(\\theta))$ The cosine function made the problem awkward and unweildy. But, if we approximate the $\\cos(\\theta) \\approx 1 + \\frac{\\theta^2}{2}$ of all things, everything just fell into place much more easily. \n",
    "\n",
    "Taylor series approximation $\\cos(\\theta) \\approx $ for a parabola, a hyperbolic \n",
    "\tfunction like $\\cosh(x)\\approx 1 + \\frac{x^2}{2}$, which gives \n",
    "\t$h = R(1 - (1 + \\frac{\\theta^2}{2})) = R \\frac{\\theta^2}{2}$\n",
    "\n",
    "An approximation like that might seem completely out of left field. If we graph these functions, they do look close to each other for small angles.\n",
    "\n",
    "![pendulum](https://raw.githubusercontent.com/kks32-courses/ce311k/master/notebooks/lectures/05_numerical_diff_taylor_series_newton_raphson/simple-pendulum.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approx cos Taylor series\n",
    "\n",
    "$$f(a)+\\frac {f'(a)}{1!} (x-a)+ \\frac{f''(a)}{2!} (x-a)^2+\\frac{f'''(a)}{3!}(x-a)^3+ \\cdots$$\n",
    "\n",
    "\n",
    "$$\\cos(x) = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} \\cdots$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Raphson\n",
    "\n",
    "The Newton-Raphson method (also known as Newton's method) is a way to quickly find a good approximation for the root of a real-valued function $f(x)=0$. It uses the idea that a continuous and differentiable function can be approximated by a straight line tangent to it. Suppose you need to find the root of a continuous, differentiable function $f(x)$, and you know the root you are looking for is near the point $x = x_0$. Then Newton's method tells us that a better approximation for the root is \n",
    "\n",
    "$$x_{1} = x_0 - \\frac{f(x_0)}{f^\\prime(x_0)}$$\n",
    "\n",
    "This process may be repeated as many times as necessary to get the desired accuracy. In general, for any $x-$value $x_n$, the next value is given by \n",
    "\n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f^\\prime(x_n)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Newton Raphson method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
